{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "070528dd-c9b1-4cbc-962e-1e7c042b1d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import time\n",
    "from statistics import mean, stdev\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a84c7297-4368-4eee-83af-f781895b3c20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: langchain in /Users/ezf0057/Library/Python/3.9/lib/python/site-packages (0.3.26)\n",
      "Requirement already satisfied: langchain-community in /Users/ezf0057/Library/Python/3.9/lib/python/site-packages (0.3.27)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.66 in /Users/ezf0057/Library/Python/3.9/lib/python/site-packages (from langchain) (0.3.70)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /Users/ezf0057/Library/Python/3.9/lib/python/site-packages (from langchain) (0.3.8)\n",
      "Requirement already satisfied: langsmith>=0.1.17 in /Users/ezf0057/Library/Python/3.9/lib/python/site-packages (from langchain) (0.4.8)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /Users/ezf0057/Library/Python/3.9/lib/python/site-packages (from langchain) (2.11.7)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /Users/ezf0057/Library/Python/3.9/lib/python/site-packages (from langchain) (2.0.41)\n",
      "Requirement already satisfied: requests<3,>=2 in /Users/ezf0057/Library/Python/3.9/lib/python/site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /Users/ezf0057/Library/Python/3.9/lib/python/site-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /Users/ezf0057/Library/Python/3.9/lib/python/site-packages (from langchain) (4.0.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /Users/ezf0057/Library/Python/3.9/lib/python/site-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Users/ezf0057/Library/Python/3.9/lib/python/site-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (1.33)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /Users/ezf0057/Library/Python/3.9/lib/python/site-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (4.12.2)\n",
      "Requirement already satisfied: packaging>=23.2 in /Users/ezf0057/Library/Python/3.9/lib/python/site-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (24.2)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Users/ezf0057/Library/Python/3.9/lib/python/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.66->langchain) (3.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/ezf0057/Library/Python/3.9/lib/python/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /Users/ezf0057/Library/Python/3.9/lib/python/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /Users/ezf0057/Library/Python/3.9/lib/python/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/ezf0057/Library/Python/3.9/lib/python/site-packages (from requests<3,>=2->langchain) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/ezf0057/Library/Python/3.9/lib/python/site-packages (from requests<3,>=2->langchain) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/ezf0057/Library/Python/3.9/lib/python/site-packages (from requests<3,>=2->langchain) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/ezf0057/Library/Python/3.9/lib/python/site-packages (from requests<3,>=2->langchain) (2025.1.31)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /Users/ezf0057/Library/Python/3.9/lib/python/site-packages (from langchain-community) (3.12.14)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /Users/ezf0057/Library/Python/3.9/lib/python/site-packages (from langchain-community) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /Users/ezf0057/Library/Python/3.9/lib/python/site-packages (from langchain-community) (2.10.1)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /Users/ezf0057/Library/Python/3.9/lib/python/site-packages (from langchain-community) (0.4.1)\n",
      "Requirement already satisfied: numpy>=1.26.2 in /Users/ezf0057/Library/Python/3.9/lib/python/site-packages (from langchain-community) (2.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /Users/ezf0057/Library/Python/3.9/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /Users/ezf0057/Library/Python/3.9/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/ezf0057/Library/Python/3.9/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/ezf0057/Library/Python/3.9/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/ezf0057/Library/Python/3.9/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.6.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Users/ezf0057/Library/Python/3.9/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Users/ezf0057/Library/Python/3.9/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.1)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /Users/ezf0057/Library/Python/3.9/lib/python/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /Users/ezf0057/Library/Python/3.9/lib/python/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in /Users/ezf0057/Library/Python/3.9/lib/python/site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.1.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /Users/ezf0057/Library/Python/3.9/lib/python/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.1.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/ezf0057/Library/Python/3.9/lib/python/site-packages (from langsmith>=0.1.17->langchain) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /Users/ezf0057/Library/Python/3.9/lib/python/site-packages (from langsmith>=0.1.17->langchain) (3.11.0)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /Users/ezf0057/Library/Python/3.9/lib/python/site-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /Users/ezf0057/Library/Python/3.9/lib/python/site-packages (from langsmith>=0.1.17->langchain) (0.23.0)\n",
      "Requirement already satisfied: anyio in /Users/ezf0057/Library/Python/3.9/lib/python/site-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (4.9.0)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/ezf0057/Library/Python/3.9/lib/python/site-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /Users/ezf0057/Library/Python/3.9/lib/python/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (0.16.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /Users/ezf0057/Library/Python/3.9/lib/python/site-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /Users/ezf0057/Library/Python/3.9/lib/python/site-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.3.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e7cb2758-22a3-47de-b83a-440981cf25a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain.schema import SystemMessage, HumanMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "001899a1-0279-4189-b485-e051779fec19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "#from chatollama import ChatOllama\n",
    "from langchain.schema import SystemMessage, HumanMessage\n",
    "\n",
    "# ---- Config ---- #\n",
    "STUDENT_MODEL = \"llama3.1:8b\"\n",
    "TEACHER_MODEL = \"llama3.1:70b\"#\n",
    "\n",
    "SCORE_WEIGHTS = {\n",
    "    \"accuracy\": 0.5,\n",
    "    \"completeness\": 0.2,\n",
    "    \"relevance\": 0.15,\n",
    "    \"clarity\": 0.15\n",
    "}\n",
    "INTERVENTION_THRESHOLD = 0.8\n",
    "SCORE_MAX = 3\n",
    "\n",
    "# ---- Penalty Function ---- #\n",
    "def should_teacher_intervene(scores, threshold=0.8):\n",
    "    \"\"\"\n",
    "    Determine if the teacher LLM should intervene based on scores.\n",
    "    \n",
    "    Args:\n",
    "        scores (dict): Dictionary with keys 'accuracy', 'completeness', 'relevance', 'clarity',\n",
    "                       each with integer score 1 to 3.\n",
    "        threshold (float): Penalty threshold for intervention.\n",
    "        \n",
    "    Returns:\n",
    "        bool: True if teacher should intervene, False otherwise.\n",
    "    \"\"\"\n",
    "    a = scores['accuracy']\n",
    "    c = scores['completeness']\n",
    "    r = scores['relevance']\n",
    "    l = scores['clarity']\n",
    "\n",
    "    # Compute weighted penalty\n",
    "    penalty = 0.5 * (3 - a) + 0.2 * (3 - c) + 0.15 * (3 - r) + 0.15 * (3 - l)\n",
    "\n",
    "    # Suspicious patterns\n",
    "    suspicious = (\n",
    "        (a == 1 and l == 3) or                          # Fluent but inaccurate\n",
    "        (a >= 2 and r == 1) or                          # Correct but off-topic\n",
    "        (a == 2 and r == 2 and l == 2) or               # Ambiguous scores\n",
    "        (a + r + l <= 5) or                             # Generally low scores\n",
    "        (abs(a - r) >= 2) or                            # Large inconsistency\n",
    "        (a == 3 and (r <= 1 or l <= 1))                 # Overconfident accuracy\n",
    "    )\n",
    "\n",
    "    # Decide intervention\n",
    "    return penalty > threshold or suspicious\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "32bdecb7-eb4e-4929-a5cd-65a3b11f43ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#------- Original prompt used by Student LLM to produce scores. Student LLM will use this prompt again after receiving teacher's feedback---#\n",
    "def call_llm_LangChain_correct_v2(prompt, temperature=0.0):\n",
    "    llm = ChatOllama(\n",
    "        model=\"llama3.1:8b\",\n",
    "        temperature=temperature\n",
    "    )\n",
    "\n",
    "    system_prompt = \"\"\"\n",
    "\n",
    "You are a large language model acting as a judge for assessing the performance of a Teaching Assistant (TA) in an introductory Python programming course.\n",
    "\n",
    "The TA is an LLM that answers student questions about Python code. Your job is to evaluate the quality of the TA's answer.\n",
    "\n",
    "You will receive:\n",
    "- A Python code snippet\n",
    "- A student question about that code\n",
    "- A reference (correct) answer\n",
    "- A TA LLM-generated answer (called the prediction)\n",
    "\n",
    "Your task is to evaluate how well the TA's prediction answers the student's question, using the following four dimensions. For each, provide:\n",
    "- An integer score from 1 to 3\n",
    "\n",
    "\n",
    "\n",
    "### Accuracy\n",
    "Compare the prediction with the reference to assess factual correctness and understanding of the code’s behavior and intent.\n",
    "You must judge whether the prediction reflects accurate behavior and matches core facts from the reference. \n",
    "You need to consider semantic meaning of code comprehension: understanding the structure, functionality, and intent behind the code.\\n\"\n",
    "\n",
    "Score meanings:\n",
    "- 1: Completely incorrect or irrelevant; does not address the reference answer.\n",
    "- 2: Partially correct; some key facts are accurate, but major details are wrong or missing.\n",
    "- 3: Fully correct; matches the reference answer in meaning and factual content.\n",
    "\n",
    "### Completeness\n",
    "Check if the prediction covers all important parts of the reference answer, including key concepts or conditions.\n",
    "\n",
    "Score meanings:\n",
    "- 1: Omits most key information or contains only a tiny fragment of relevant content.\n",
    "- 2: Covers some elements but misses important parts.\n",
    "- 3: Fully covers all essential information from the reference.\n",
    "\n",
    "### Relevance\n",
    "Assess whether the prediction directly addresses the question and stays on-topic.\n",
    "\n",
    "Score meanings:\n",
    "- 1: Completely irrelevant or mostly unrelated.\n",
    "- 2: Partially related but misses the main point.\n",
    "- 3: Fully focused and directly answers the question.\n",
    "\n",
    "### Clarity\n",
    "Evaluate how clearly and logically the prediction is expressed, ensuring it is easy to understand.\n",
    "\n",
    "Score meanings:\n",
    "- 1: Confusing, vague, or incoherent.\n",
    "- 2: Understandable but awkwardly phrased or slightly unclear.\n",
    "- 3: Clear, concise, and easy to follow.\n",
    "\n",
    "\n",
    "Example:\n",
    "\n",
    "Code:\n",
    "```python\n",
    "def count_even(nums):\n",
    "    total = 0\n",
    "    for x in nums:\n",
    "        if x % 2 == 0:\n",
    "            total += 1\n",
    "    return total\n",
    "Question: What does this function return when given a list of integers?\n",
    "Reference Answer: It returns the count of even numbers in the list.\n",
    "Prediction: It returns the count of odd numbers in the list.\n",
    "\n",
    "Evaluation Output:\n",
    "{\n",
    "\n",
    "\"accuracy\": { \"score\": 1 },\n",
    "\"completeness\": { \"score\": 1 },\n",
    "\"relevance\": { \"score\": 2 },\n",
    "\"clarity\": { \"score\": 3 }\n",
    "\n",
    "}\n",
    "\n",
    "Final Instructions:\n",
    "For the given input (code, question, reference answer, and prediction), evaluate the prediction on the four metrics defined above.\n",
    "Base your evaluation strictly on the content provided. Do not hallucinate missing information. Be consistent and objective.\n",
    "Do not include reasoning or explanations.\n",
    "\n",
    "Respond only with a JSON object in the exact format:\n",
    "{\n",
    "\"accuracy\": { \"score\": 1-3},\n",
    "\"completeness\": {\"score\": 1-3},\n",
    "\"relevance\": {\"score\": 1-3},\n",
    "\"clarity\": {\"score\": 1-3}\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "  \n",
    "\n",
    "    messages = [\n",
    "        SystemMessage(content=system_prompt),\n",
    "        HumanMessage(content=prompt)\n",
    "    ]\n",
    "\n",
    "    response = llm.invoke(messages)\n",
    "    return response.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a578c02e-f89d-49ac-ae0e-cb2cfaef15c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Step 3: Student provides reasoning after teacher prompts ---- #\n",
    "def generate_reasoning_after_teacher_prompt(code, question, reference, prediction, scores):\n",
    "    reasonings = {}\n",
    "    for dim, score in scores.items():\n",
    "        prompt = f\"\"\"\n",
    "The teacher is requesting your reasoning for why you scored {score}/3 on {dim.upper()}.\n",
    "\n",
    "Explain briefly (1–2 sentences) based on the predicted answer vs. the reference.\n",
    "\"\"\"\n",
    "        full_input = f\"\"\"\n",
    "Code:\n",
    "```python\n",
    "{code}\n",
    "Question: {question}\n",
    "\n",
    "Reference Answer:\n",
    "{reference}\n",
    "\n",
    "Predicted Answer:\n",
    "{prediction}\n",
    "\"\"\"\n",
    "        llm = ChatOllama(model=STUDENT_MODEL, temperature=0.3)\n",
    "        messages = [SystemMessage(content=prompt), HumanMessage(content=full_input)]\n",
    "        response = llm.invoke(messages)\n",
    "        reasonings[dim] = response.content.strip()\n",
    "    return reasonings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eddbbdf7-ade3-4742-9b80-61d32184c56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Step 4: Teacher critiques ---- #\n",
    "def teacher_critiques_student(code, question, reference, prediction, reasonings, scores):\n",
    "    critiques = {}\n",
    "    for dim, reasoning in reasonings.items():\n",
    "        score = scores.get(dim, SCORE_MAX)\n",
    "        if score < SCORE_MAX:\n",
    "            system_prompt = f\"\"\"\n",
    "You are a 70B teacher model. The student scored {score}/3 for {dim.upper()} and gave this reasoning:\n",
    "\\\"{reasoning}\\\"\n",
    "\n",
    "\n",
    "You will receive:\n",
    "- Python code snippet\n",
    "- Student's question\n",
    "- Reference (correct) answer\n",
    "- TA's predicted answer\n",
    "- Scores assigned by the student LLM-as-judge\n",
    "- Reasoning \n",
    "\n",
    "Your task:\n",
    "- Examine the TA's predicted answer in context of the code, question, and reference.\n",
    "- For any dimension (Accuracy, Completeness, Relevance, Clarity) where the score is less than 3,\n",
    "  provide clear, concise feedback (2–4 sentences) explaining what could be improved.\n",
    "- If a dimension has no issues, do not include it in your response.\n",
    "- Critique the reasoning in 3–5 sentences. Point out flaws and suggest improvement.\n",
    "\n",
    "Respond ONLY with a JSON object where keys are the dimension names (lowercase)\n",
    "and values are the feedback strings.\n",
    "\n",
    "Example output:\n",
    "{{\n",
    "  \"accuracy\": \"The prediction misrepresents the function’s return value.\",\n",
    "  \"clarity\": \"The explanation lacks structure and is hard to follow.\"\n",
    "}}\n",
    "\n",
    "Rubric:\n",
    "\n",
    "### Accuracy\n",
    "- 1: Completely incorrect or irrelevant.\n",
    "- 2: Partially correct but with major mistakes or omissions.\n",
    "- 3: Fully correct and matches the reference.\n",
    "\n",
    "### Completeness\n",
    "- 1: Omits most key information.\n",
    "- 2: Covers some but misses important parts.\n",
    "- 3: Fully covers all essential information.\n",
    "\n",
    "### Relevance\n",
    "- 1: Irrelevant or mostly unrelated.\n",
    "- 2: Partially related but misses main point.\n",
    "- 3: Fully focused and directly addresses the question.\n",
    "\n",
    "### Clarity\n",
    "- 1: Confusing, vague, or incoherent.\n",
    "- 2: Understandable but awkwardly phrased or unclear.\n",
    "- 3: Clear, concise, and easy to understand.\n",
    "\"\"\"\n",
    "            user_prompt = (\n",
    "                \"Code:\\n\"\n",
    "                \"```python\\n\"\n",
    "                f\"{code}\\n\"\n",
    "                \"```\\n\"\n",
    "                f\"Question: {question}\\n\\n\"\n",
    "                f\"Reference Answer:\\n{reference}\\n\\n\"\n",
    "                f\"Predicted Answer:\\n{prediction}\"\n",
    "            )\n",
    "\n",
    "            llm = ChatOllama(model=TEACHER_MODEL, temperature=0.0)\n",
    "            messages = [\n",
    "                SystemMessage(content=system_prompt),\n",
    "                HumanMessage(content=user_prompt)\n",
    "            ]\n",
    "            response = llm.invoke(messages)\n",
    "            critiques[dim] = response.content.strip()\n",
    "    return critiques\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "43a85fbe-a0e6-43b7-bd53-70ceeec3fc69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Step 5: Student Judge LLM reflects an revises  ---- #\n",
    "def student_reflect_and_revise(code, question, reference, prediction, old_score, critiques):\n",
    "    print(\"\\n Student reflecting on teacher feedback...\\n\")\n",
    "\n",
    "    critique_text = \"\\n\".join(\n",
    "        f\"{dim.upper()} Feedback: {critique.strip()}\" for dim, critique in critiques.items()\n",
    "    )\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "Code:\n",
    "```python\n",
    "{code}\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Reference Answer:\n",
    "{reference}\n",
    "\n",
    "TA's Predicted Answer:\n",
    "{prediction}\n",
    "\n",
    "Teacher's Feedback:\n",
    "{critique_text}\n",
    "\n",
    "Old prediction:\n",
    "{old_score}\n",
    "\n",
    "You have to update the old prediction by considering Teacher's Feedback. Please re-evaluate the TA's answer using:\n",
    "\n",
    "Accuracy\n",
    "\n",
    "Completeness\n",
    "\n",
    "Relevance\n",
    "\n",
    "Clarity\n",
    "\n",
    "Only return JSON:\n",
    "{{\n",
    "\"accuracy\": {{ \"score\": 1-3 }},\n",
    "\"completeness\": {{ \"score\": 1-3 }},\n",
    "\"relevance\": {{ \"score\": 1-3 }},\n",
    "\"clarity\": {{ \"score\": 1-3 }}\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "    try:\n",
    "        response = call_llm_LangChain_correct_v2(prompt, temperature=0.0)\n",
    "        revised_scores = json.loads(response)\n",
    "        print(\" Revised Scores:\\n\", revised_scores)\n",
    "    except Exception as e:\n",
    "        print(\" Failed to parse revised scores:\", str(e))\n",
    "        revised_scores = None\n",
    "\n",
    "    return revised_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0ea0e590-94d8-4dde-a7e0-ffe216b70e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Step 1: Student Judge LLM provides scores  ---- #\n",
    "\n",
    "example = {\"id\": \"q132\",\n",
    "   \"code\": \"from cs1robots import* load_world(\\\"worlds/harvest1.wld\\\") #load world(\\\"worlds/harvest3.wld\\\") #load world(\\\"worlds/harvest4.wld\\\") hubo=Robot() hubo.set_trace('blue') def harvest(): if hubo.on_beeper(): hubo.pick_beeper() else: hubo.move() def right(): for i in range(3): hubo.turn_left() def right_up(): if hubo.on_beeper(): hubo.pick_beeper() hubo.turn_left() hubo.move() hubo.turn_left() def left_up(): right() hubo.move() right() def loop(): while hubo.front_is_clear(): harvest() right_up() while hubo.front_is_clear(): harvest() for i in range(2): loop() left_up() loop()\",\n",
    "    \"question\": \"In task4, do I have to pass only the beeper?\",\n",
    "    \"reference\": \"No, you don't need to, just pick up all the beepers!\",\n",
    "    \"prediction\": \"\\nHello! I understand that you are asking about a specific task related to programming in Python. However, your question is quite broad and doesn't provide enough context for me to fully understand what you need help with. Could you please provide more details or clarify your question so that I can better assist you?\",\n",
    "  }\n",
    "studentLLM_score = {\"accuracy\": 1, \"completeness\": 2, \"relevance\": 1, \"clarity\": 3}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e2b0e448-8a48-4d80-a5ac-67b0e8c9fe8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Step 2: Decide whether the teacher LLM will intervene  ---- #\n",
    "do_intervene = should_teacher_intervene(studentLLM_score, threshold=0.8)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ec6f525e-b9bc-4300-83ac-ce3feab75fcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(do_intervene)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a1450b-15d5-4c55-b4f8-f5177b1b805e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reasoning  {'accuracy': \"Here's a brief explanation of why the predicted answer scored 1/3 on ACCURACY:\\n\\nThe predicted answer was a generic response that didn't address the specific task at hand, whereas the reference answer provided a clear and concise solution to the problem. The predicted answer failed to understand the context of the question and provide a relevant response, resulting in low accuracy.\", 'completeness': \"Here's a brief explanation for scoring 2/3 on COMPLETENESS:\\n\\nI scored low on completeness because my predicted answer failed to directly address the specific task of Task4, instead providing a generic response about understanding the context. A more complete response would have explicitly stated that the robot is designed to pick up all beepers in its path, not just pass them by.\", 'relevance': \"Here's a brief explanation of why the predicted answer scored 1/3 on RELEVANCE:\\n\\nThe predicted answer is off-topic and doesn't address the specific task in question (task4). It appears to be a generic response asking for more context, rather than providing a relevant solution or insight into the task.\", 'clarity': \"Here's a brief explanation of my reasoning:\\n\\nI scored 3/3 on CLARITY because my response provided a clear and direct answer to the question, while also showing an understanding of the task requirements. My response acknowledged that the original question was too broad, but then directly addressed the specific query about whether only one beeper needs to be passed in Task4.\"}\n",
      "critices  {'accuracy': '{\\n  \"accuracy\": \"The predicted answer does not address the specific task at hand and instead asks for clarification, whereas the reference answer provides a clear and concise solution to the problem.\",\\n  \"relevance\": \"The predicted answer is mostly unrelated to the question asked, failing to provide any relevant information or insight into the task.\"\\n}\\n\\nCritique of reasoning:\\nThe student\\'s reasoning correctly identifies that the predicted answer is generic and does not address the specific task. However, it could be improved by providing more specific examples from the code or reference answer to support their claim. Additionally, the student could have also mentioned the relevance dimension as an issue, as the predicted answer fails to provide any relevant information.', 'completeness': '{\\n  \"completeness\": \"The predicted answer does not address the specific task of Task4 at all, instead asking for clarification on the question. A complete response would have directly answered the question based on the provided code and context.\",\\n  \"relevance\": \"The predicted answer is partially related but misses the main point of the question. It fails to provide a relevant response to the specific task of Task4.\",\\n  \"clarity\": \"The explanation is clear in asking for clarification, but it does not address the actual question, making it unclear and unhelpful in resolving the query about Task4.\"\\n}\\n\\nNote: The student\\'s reasoning is flawed as they mention that their predicted answer failed to directly address the specific task of Task4, which is correct. However, they also state that a more complete response would have explicitly stated that the robot is designed to pick up all beepers in its path, not just pass them by. This is incorrect because the predicted answer does not even attempt to address this point, instead asking for clarification on the question.', 'relevance': '{\\n  \"relevance\": \"The predicted answer does not address the specific task in question (task4) and instead asks for more context, which is not relevant to providing a solution or insight into the task. To improve, focus on understanding the task requirements and provide a direct response.\",\\n  \"completeness\": \"The predicted answer omits all key information related to the task, as it does not attempt to address the question at all. A more complete response would acknowledge the task requirements and provide relevant guidance or solutions.\"\\n}'}\n",
      "\n",
      " Student reflecting on teacher feedback...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "teacher_feedbacks =0\n",
    "if (do_intervene):\n",
    "    #example[\"code\"], example[\"question\"], example[\"reference\"], example[\"prediction\"], studentLLM_score\n",
    "    reasoning_std = generate_reasoning_after_teacher_prompt(example[\"code\"], example[\"question\"], \n",
    "                                    example[\"reference\"], example[\"prediction\"], studentLLM_score)\n",
    "    print(\"reasoning \", reasoning_std)\n",
    "    critiques= teacher_critiques_student(example[\"code\"], example[\"question\"], example[\"reference\"], example[\"prediction\"], reasoning_std, studentLLM_score)\n",
    "    print(\"critices \", critiques)\n",
    "    revised_scores = student_reflect_and_revise(example[\"code\"], example[\"question\"], example[\"reference\"], example[\"prediction\"], studentLLM_score, critiques)\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f7724b-b4d8-4d09-8c31-0743a0881da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#results = run_multi_turn_pipeline(example)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
