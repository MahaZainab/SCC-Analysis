{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91fad3d2-06b2-4bfe-aadb-07c0cc48f11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import time\n",
    "from statistics import mean, stdev\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🔧 Added by assistant: JSON dataset loader & runner\n",
    "These cells load `Llama3_CodeQA_llm_as_judge.json` and provide a runner to iterate examples.\n",
    "If your notebook defines a function like `run_example(example, studentLLM_score)`, the runner will call it for each record.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 21 examples from Llama3_CodeQA_llm_as_judge.json\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# === Added: JSON loader (Llama3_CodeQA_llm_as_judge.json) ===\n",
    "from pathlib import Path as _Path\n",
    "import json as _json\n",
    "\n",
    "_JSON_PATH = _Path(\"Llama3_CodeQA_llm_as_judge.json\")\n",
    "\n",
    "def _extract_scores(_rec):\n",
    "    def _get_score(_k):\n",
    "        _v = _rec.get(_k, {})\n",
    "        if isinstance(_v, dict):\n",
    "            return int(_v.get(\"score\", 0))\n",
    "        if isinstance(_v, (int, float)):\n",
    "            return int(_v)\n",
    "        return 0\n",
    "    return {\n",
    "        \"accuracy\": _get_score(\"accuracy\"),\n",
    "        \"completeness\": _get_score(\"completeness\"),\n",
    "        \"relevance\": _get_score(\"relevance\"),\n",
    "        \"clarity\": _get_score(\"clarity\"),\n",
    "    }\n",
    "\n",
    "with _JSON_PATH.open(\"r\", encoding=\"utf-8\") as _f:\n",
    "    _data = _json.load(_f)\n",
    "\n",
    "if not isinstance(_data, list):\n",
    "    raise ValueError(\"Expected top-level list in Llama3_CodeQA_llm_as_judge.json\")\n",
    "\n",
    "# DATASET: list of (example, studentLLM_score) tuples\n",
    "DATASET = []\n",
    "for _rec in _data:\n",
    "    _example = {\n",
    "        \"id\": _rec.get(\"id\"),\n",
    "        \"code\": _rec.get(\"code\"),\n",
    "        \"question\": _rec.get(\"question\"),\n",
    "        \"reference\": _rec.get(\"reference\"),\n",
    "        \"prediction\": _rec.get(\"prediction\"),\n",
    "    }\n",
    "    _scores = _extract_scores(_rec)\n",
    "    DATASET.append((_example, _scores))\n",
    "\n",
    "print(f\"Loaded {len(DATASET)} examples from {_JSON_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4134ca7c-0ac9-40ac-bd73-aaf78483ff35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: langchain in c:\\users\\pc\\appdata\\roaming\\python\\python312\\site-packages (0.3.26)\n",
      "Requirement already satisfied: langchain-community in c:\\users\\pc\\appdata\\roaming\\python\\python312\\site-packages (0.3.27)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.66 in c:\\users\\pc\\appdata\\roaming\\python\\python312\\site-packages (from langchain) (0.3.68)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in c:\\users\\pc\\appdata\\roaming\\python\\python312\\site-packages (from langchain) (0.3.8)\n",
      "Requirement already satisfied: langsmith>=0.1.17 in c:\\users\\pc\\appdata\\roaming\\python\\python312\\site-packages (from langchain) (0.4.4)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\pc\\appdata\\roaming\\python\\python312\\site-packages (from langchain) (2.11.7)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\pc\\appdata\\roaming\\python\\python312\\site-packages (from langchain) (2.0.32)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\pc\\appdata\\roaming\\python\\python312\\site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\pc\\appdata\\roaming\\python\\python312\\site-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\pc\\appdata\\roaming\\python\\python312\\site-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (8.5.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\pc\\appdata\\roaming\\python\\python312\\site-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\pc\\appdata\\roaming\\python\\python312\\site-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\pc\\appdata\\roaming\\python\\python312\\site-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (4.13.2)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\pc\\appdata\\roaming\\python\\python312\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.66->langchain) (3.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\pc\\appdata\\roaming\\python\\python312\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\pc\\appdata\\roaming\\python\\python312\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\pc\\appdata\\roaming\\python\\python312\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\pc\\appdata\\roaming\\python\\python312\\site-packages (from requests<3,>=2->langchain) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\pc\\appdata\\roaming\\python\\python312\\site-packages (from requests<3,>=2->langchain) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\pc\\appdata\\roaming\\python\\python312\\site-packages (from requests<3,>=2->langchain) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\pc\\appdata\\roaming\\python\\python312\\site-packages (from requests<3,>=2->langchain) (2025.4.26)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\pc\\appdata\\roaming\\python\\python312\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\pc\\appdata\\roaming\\python\\python312\\site-packages (from langchain-community) (3.10.3)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in c:\\users\\pc\\appdata\\roaming\\python\\python312\\site-packages (from langchain-community) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in c:\\users\\pc\\appdata\\roaming\\python\\python312\\site-packages (from langchain-community) (2.10.1)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in c:\\users\\pc\\appdata\\roaming\\python\\python312\\site-packages (from langchain-community) (0.4.1)\n",
      "Requirement already satisfied: numpy>=1.26.2 in c:\\users\\pc\\appdata\\roaming\\python\\python312\\site-packages (from langchain-community) (2.1.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\pc\\appdata\\roaming\\python\\python312\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.3.5)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\pc\\appdata\\roaming\\python\\python312\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\pc\\appdata\\roaming\\python\\python312\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\pc\\appdata\\roaming\\python\\python312\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\pc\\appdata\\roaming\\python\\python312\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\pc\\appdata\\roaming\\python\\python312\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.9.4)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\pc\\appdata\\roaming\\python\\python312\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.21.3)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\pc\\appdata\\roaming\\python\\python312\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in c:\\users\\pc\\appdata\\roaming\\python\\python312\\site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.0.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\pc\\appdata\\roaming\\python\\python312\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\pc\\appdata\\roaming\\python\\python312\\site-packages (from langsmith>=0.1.17->langchain) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\pc\\appdata\\roaming\\python\\python312\\site-packages (from langsmith>=0.1.17->langchain) (3.10.7)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\pc\\appdata\\roaming\\python\\python312\\site-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in c:\\users\\pc\\appdata\\roaming\\python\\python312\\site-packages (from langsmith>=0.1.17->langchain) (0.23.0)\n",
      "Requirement already satisfied: anyio in c:\\users\\pc\\appdata\\roaming\\python\\python312\\site-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (4.7.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\pc\\appdata\\roaming\\python\\python312\\site-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\pc\\appdata\\roaming\\python\\python312\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (0.14.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\pc\\appdata\\roaming\\python\\python312\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.3.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85da4fcb-5bff-4999-9710-6779a6e3d62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain.schema import SystemMessage, HumanMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3141636b-ca2d-4f2e-8a05-359cedbe6f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def should_teacher_intervene(scores, threshold=0.8):\n",
    "    \"\"\"\n",
    "    Determine if the teacher LLM should intervene based on scores.\n",
    "    \n",
    "    Args:\n",
    "        scores (dict): Dictionary with keys 'accuracy', 'completeness', 'relevance', 'clarity',\n",
    "                       each with integer score 1 to 3.\n",
    "        threshold (float): Penalty threshold for intervention.\n",
    "        \n",
    "    Returns:\n",
    "        bool: True if teacher should intervene, False otherwise.\n",
    "    \"\"\"\n",
    "    a = scores['accuracy']\n",
    "    c = scores['completeness']\n",
    "    r = scores['relevance']\n",
    "    l = scores['clarity']\n",
    "\n",
    "    # Compute weighted penalty\n",
    "    penalty = 0.5 * (3 - a) + 0.2 * (3 - c) + 0.15 * (3 - r) + 0.15 * (3 - l)\n",
    "\n",
    "    # Suspicious patterns\n",
    "    suspicious = (\n",
    "        (a == 1 and l == 3) or                          # Fluent but inaccurate\n",
    "        (a >= 2 and r == 1) or                          # Correct but off-topic\n",
    "        (a == 2 and r == 2 and l == 2) or               # Ambiguous scores\n",
    "        (a + r + l <= 5) or                             # Generally low scores\n",
    "        (abs(a - r) >= 2) or                            # Large inconsistency\n",
    "        (a == 3 and (r <= 1 or l <= 1))                 # Overconfident accuracy\n",
    "    )\n",
    "\n",
    "    # Decide intervention\n",
    "    return penalty > threshold or suspicious\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e9a4a43-df27-4f3a-9d9d-c863e446991a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "\n",
    "STUDENT_MODEL = \"llama3.1:8b\"\n",
    "TEACHER_MODEL = \"llama3.1:8b\"\n",
    "SCORE_MAX = 3\n",
    "\n",
    "\n",
    "def call_llm_LangChain_correct_v2(prompt, temperature=0.0):\n",
    "    llm = ChatOllama(\n",
    "        model=\"llama3.1:8b\",\n",
    "        temperature=temperature\n",
    "    )\n",
    "\n",
    "    system_prompt = \"\"\"\n",
    "\n",
    "You are a large language model acting as a judge for assessing the performance of a Teaching Assistant (TA) in an introductory Python programming course.\n",
    "\n",
    "The TA is an LLM that answers student questions about Python code. Your job is to evaluate the quality of the TA's answer.\n",
    "\n",
    "You will receive:\n",
    "- A Python code snippet\n",
    "- A student question about that code\n",
    "- A reference (correct) answer\n",
    "- A TA LLM-generated answer (called the prediction)\n",
    "\n",
    "Your task is to evaluate how well the TA's prediction answers the student's question, using the following four dimensions. For each, provide:\n",
    "- An integer score from 1 to 3\n",
    "\n",
    "\n",
    "\n",
    "### Accuracy\n",
    "Compare the prediction with the reference to assess factual correctness and understanding of the code’s behavior and intent.\n",
    "You must judge whether the prediction reflects accurate behavior and matches core facts from the reference. \n",
    "You need to consider semantic meaning of code comprehension: understanding the structure, functionality, and intent behind the code.\\n\"\n",
    "\n",
    "Score meanings:\n",
    "- 1: Completely incorrect or irrelevant; does not address the reference answer.\n",
    "- 2: Partially correct; some key facts are accurate, but major details are wrong or missing.\n",
    "- 3: Fully correct; matches the reference answer in meaning and factual content.\n",
    "\n",
    "### Completeness\n",
    "Check if the prediction covers all important parts of the reference answer, including key concepts or conditions.\n",
    "\n",
    "Score meanings:\n",
    "- 1: Omits most key information or contains only a tiny fragment of relevant content.\n",
    "- 2: Covers some elements but misses important parts.\n",
    "- 3: Fully covers all essential information from the reference.\n",
    "\n",
    "### Relevance\n",
    "Assess whether the prediction directly addresses the question and stays on-topic.\n",
    "\n",
    "Score meanings:\n",
    "- 1: Completely irrelevant or mostly unrelated.\n",
    "- 2: Partially related but misses the main point.\n",
    "- 3: Fully focused and directly answers the question.\n",
    "\n",
    "### Clarity\n",
    "Evaluate how clearly and logically the prediction is expressed, ensuring it is easy to understand.\n",
    "\n",
    "Score meanings:\n",
    "- 1: Confusing, vague, or incoherent.\n",
    "- 2: Understandable but awkwardly phrased or slightly unclear.\n",
    "- 3: Clear, concise, and easy to follow.\n",
    "\n",
    "\n",
    "Example:\n",
    "\n",
    "Code:\n",
    "```python\n",
    "def count_even(nums):\n",
    "    total = 0\n",
    "    for x in nums:\n",
    "        if x % 2 == 0:\n",
    "            total += 1\n",
    "    return total\n",
    "Question: What does this function return when given a list of integers?\n",
    "Reference Answer: It returns the count of even numbers in the list.\n",
    "Prediction: It returns the count of odd numbers in the list.\n",
    "\n",
    "Evaluation Output:\n",
    "{\n",
    "\n",
    "\"accuracy\": { \"score\": 1 },\n",
    "\"completeness\": { \"score\": 1 },\n",
    "\"relevance\": { \"score\": 2 },\n",
    "\"clarity\": { \"score\": 3 }\n",
    "\n",
    "}\n",
    "\n",
    "Final Instructions:\n",
    "For the given input (code, question, reference answer, and prediction), evaluate the prediction on the four metrics defined above.\n",
    "Base your evaluation strictly on the content provided. Do not hallucinate missing information. Be consistent and objective.\n",
    "Do not include reasoning or explanations.\n",
    "\n",
    "Respond only with a JSON object in the exact format:\n",
    "{\n",
    "\"accuracy\": { \"score\": 1-3},\n",
    "\"completeness\": {\"score\": 1-3},\n",
    "\"relevance\": {\"score\": 1-3},\n",
    "\"clarity\": {\"score\": 1-3}\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "  \n",
    "\n",
    "    messages = [\n",
    "        SystemMessage(content=system_prompt),\n",
    "        HumanMessage(content=prompt)\n",
    "    ]\n",
    "\n",
    "    response = llm.invoke(messages)\n",
    "    return response.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d524b42-b629-4023-8d27-223ac20217c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_teacher_single_turn(code, question, reference, prediction, scores):\n",
    "    # Build teacher system prompt\n",
    "    system_prompt = \"\"\"\n",
    "You are a 70B teacher LLM model reviewing a student LLM-as-judge's evaluation of a Teaching Assistant's (TA) answer.\n",
    "\n",
    "You will receive:\n",
    "- Python code snippet\n",
    "- Student's question\n",
    "- Reference (correct) answer\n",
    "- TA's predicted answer\n",
    "- Scores assigned by the student LLM-as-judge\n",
    "\n",
    "Your task:\n",
    "- Examine the TA's predicted answer in context of the code, question, and reference.\n",
    "- For any dimension (Accuracy, Completeness, Relevance, Clarity) where the score is less than 3,\n",
    "  provide clear, concise feedback (2–4 sentences) explaining what could be improved.\n",
    "- If a dimension has no issues, do not include it in your response.\n",
    "\n",
    "Respond ONLY with a JSON object where keys are the dimension names (lowercase)\n",
    "and values are the feedback strings.\n",
    "\n",
    "Example output:\n",
    "{\n",
    "  \"accuracy\": \"The prediction misrepresents the function’s return value.\",\n",
    "  \"clarity\": \"The explanation lacks structure and is hard to follow.\"\n",
    "}\n",
    "\n",
    "Rubric:\n",
    "\n",
    "### Accuracy\n",
    "- 1: Completely incorrect or irrelevant.\n",
    "- 2: Partially correct but with major mistakes or omissions.\n",
    "- 3: Fully correct and matches the reference.\n",
    "\n",
    "### Completeness\n",
    "- 1: Omits most key information.\n",
    "- 2: Covers some but misses important parts.\n",
    "- 3: Fully covers all essential information.\n",
    "\n",
    "### Relevance\n",
    "- 1: Irrelevant or mostly unrelated.\n",
    "- 2: Partially related but misses main point.\n",
    "- 3: Fully focused and directly addresses the question.\n",
    "\n",
    "### Clarity\n",
    "- 1: Confusing, vague, or incoherent.\n",
    "- 2: Understandable but awkwardly phrased or unclear.\n",
    "- 3: Clear, concise, and easy to understand.\n",
    "\"\"\"\n",
    "\n",
    "    # Format input for the teacher\n",
    "    user_prompt = f\"\"\"\n",
    "```python\n",
    "{code}\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Reference Answer:\n",
    "{reference}\n",
    "\n",
    "TA's Predicted Answer:\n",
    "{prediction}\n",
    "\"\"\"\n",
    "\n",
    "    try:\n",
    "        llm = ChatOllama(model=TEACHER_MODEL, temperature=0.0)\n",
    "        messages = [\n",
    "            SystemMessage(content=system_prompt.strip()),\n",
    "            HumanMessage(content=user_prompt.strip())\n",
    "        ]\n",
    "        response = llm.invoke(messages)\n",
    "        critiques = json.loads(response.content.strip())\n",
    "        return critiques\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")\n",
    "        return {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5467df22-6626-40d0-a393-416770599575",
   "metadata": {},
   "outputs": [],
   "source": [
    "def student_reflect_and_revise(code, question, reference, prediction, old_score, critiques):\n",
    "    print(\"\\n Student reflecting on teacher feedback...\\n\")\n",
    "\n",
    "    critique_text = \"\\n\".join(\n",
    "        f\"{dim.upper()} Feedback: {critique.strip()}\" for dim, critique in critiques.items()\n",
    "    )\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "Code:\n",
    "```python\n",
    "{code}\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Reference Answer:\n",
    "{reference}\n",
    "\n",
    "TA's Predicted Answer:\n",
    "{prediction}\n",
    "\n",
    "Teacher's Feedback:\n",
    "{critique_text}\n",
    "\n",
    "Old prediction:\n",
    "{old_score}\n",
    "\n",
    "You have to update the old prediction by considering Teacher's Feedback. Please re-evaluate the TA's answer using:\n",
    "\n",
    "Accuracy\n",
    "\n",
    "Completeness\n",
    "\n",
    "Relevance\n",
    "\n",
    "Clarity\n",
    "\n",
    "Only return JSON:\n",
    "{{\n",
    "\"accuracy\": {{ \"score\": 1-3 }},\n",
    "\"completeness\": {{ \"score\": 1-3 }},\n",
    "\"relevance\": {{ \"score\": 1-3 }},\n",
    "\"clarity\": {{ \"score\": 1-3 }}\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "    try:\n",
    "        response = call_llm_LangChain_correct_v2(prompt, temperature=0.0)\n",
    "        revised_scores = json.loads(response)\n",
    "        print(\" Revised Scores:\\n\", revised_scores)\n",
    "    except Exception as e:\n",
    "        print(\" Failed to parse revised scores:\", str(e))\n",
    "        revised_scores = None\n",
    "\n",
    "    return revised_scores\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e6c9a95b-4a3c-44b6-97bf-529202b93784",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### SINGLE-TURN PIPELINE ###\n"
     ]
    }
   ],
   "source": [
    "print(\"### SINGLE-TURN PIPELINE ###\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d86bdaef-c43a-4482-aa0e-c2de9f0cbbce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Student judge model outputs scores\n",
    "# The output is from CS1QA dataset on CodeLlama\n",
    "#)\n",
    "\n",
    "\n",
    "# example = {\"id\": \"q132\",\n",
    "#    \"code\": \"from cs1robots import* load_world(\\\"worlds/harvest1.wld\\\") #load world(\\\"worlds/harvest3.wld\\\") #load world(\\\"worlds/harvest4.wld\\\") hubo=Robot() hubo.set_trace('blue') def harvest(): if hubo.on_beeper(): hubo.pick_beeper() else: hubo.move() def right(): for i in range(3): hubo.turn_left() def right_up(): if hubo.on_beeper(): hubo.pick_beeper() hubo.turn_left() hubo.move() hubo.turn_left() def left_up(): right() hubo.move() right() def loop(): while hubo.front_is_clear(): harvest() right_up() while hubo.front_is_clear(): harvest() for i in range(2): loop() left_up() loop()\",\n",
    "#     \"question\": \"In task4, do I have to pass only the beeper?\",\n",
    "#     \"reference\": \"No, you don't need to, just pick up all the beepers!\",\n",
    "#     \"prediction\": \"\\nHello! I understand that you are asking about a specific task related to programming in Python. However, your question is quite broad and doesn't provide enough context for me to fully understand what you need help with. Could you please provide more details or clarify your question so that I can better assist you?\",\n",
    "#   }\n",
    "# studentLLM_score = {\"accuracy\": 1, \"completeness\": 2, \"relevance\": 1, \"clarity\": 3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ccef3ac5-24c5-4e55-a588-0851098ff0c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC\\AppData\\Local\\Temp\\ipykernel_7104\\532388561.py:66: LangChainDeprecationWarning: The class `ChatOllama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import ChatOllama``.\n",
      "  llm = ChatOllama(model=TEACHER_MODEL, temperature=0.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[q1] feedback:\n",
      " - completeness: The predicted answer is missing the crucial detail that it should be a 'set' of test cases, not just any test cases.\n",
      " - relevance: Although the predicted answer is related to the question, it doesn't directly address what the code makes. It's more about what the function returns or generates.\n",
      "\n",
      "[q2] no intervention.\n",
      "\n",
      "[q3] no intervention.\n",
      "\n",
      "[q4] feedback:\n",
      " - completeness: The TA's answer is partially correct but misses important details. It should specify that the received messages are from a pull subscription.\n",
      " - relevance: The TA's answer is mostly relevant to the question but could be more direct and focused on the specific context of a pull subscription.\n",
      "\n",
      "[q5] feedback:\n",
      " - completeness: The TA's answer lacks essential information about how the explicit budget is used, specifically in creating a campaign.\n",
      " - relevance: The TA's answer partially addresses the question but misses the main point of how an explicit budget is utilized.\n",
      "\n",
      "[q6] no intervention.\n",
      "\n",
      "[q7] no intervention.\n",
      "\n",
      "[q8] no intervention.\n",
      "\n",
      "[q9] no intervention.\n",
      "\n",
      "[q10] feedback:\n",
      " - accuracy: The prediction misrepresents the function’s return value; it should be 'an image' instead of just 'an image file'.\n",
      " - completeness: The answer lacks essential information about what is written to the file, specifically that it's an image.\n",
      " - relevance: The answer directly addresses the question but could be more precise in its wording.\n",
      "\n",
      "[q11] no intervention.\n",
      "\n",
      "[q12] feedback:\n",
      " - completeness: The answer is missing the context of what exactly is being added, it should specify that parameters are being added to the URL.\n",
      " - clarity: The answer could be more concise and clear in its phrasing, 'parameters' alone might not convey the full meaning intended by the question.\n",
      "\n",
      "[q13] no intervention.\n",
      "\n",
      "[q14] no intervention.\n",
      "\n",
      "[q15] no intervention.\n",
      "\n",
      "[q16] no intervention.\n",
      "\n",
      "[q17] no intervention.\n",
      "\n",
      "[q18] no intervention.\n",
      "\n",
      "[q19] feedback:\n",
      " - completeness: The answer lacks a conditional statement or context, making it incomplete.\n",
      " - relevance: The answer is partially related but misses the main point of the question, which asks for a condition.\n",
      " - clarity: The answer is concise but lacks clarity due to its simplicity and lack of explanation.\n",
      "\n",
      "[q20] feedback:\n",
      " - accuracy: The prediction misrepresents the function’s return value. It should be a boolean True/False, not 'Yes'.\n",
      " - completeness: The answer lacks essential information. It only provides a partial response to the question.\n",
      " - relevance: The answer is partially related but misses the main point. The focus should be on what we are using when, not just confirming it's windows.\n",
      "\n",
      "[q21] no intervention.\n",
      "\n",
      "Processed 21 examples.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Step 2 (batch): teacher checks & feedback over the whole JSON dataset, with compact output\n",
    "\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "JSON_PATH = Path(\"Llama3_CodeQA_llm_as_judge.json\")\n",
    "\n",
    "def _extract_scores(rec):\n",
    "    def _get(k):\n",
    "        v = rec.get(k, {})\n",
    "        if isinstance(v, dict):\n",
    "            return int(v.get(\"score\", 0))\n",
    "        if isinstance(v, (int, float)):\n",
    "            return int(v)\n",
    "        return 0\n",
    "    return {\n",
    "        \"accuracy\": _get(\"accuracy\"),\n",
    "        \"completeness\": _get(\"completeness\"),\n",
    "        \"relevance\": _get(\"relevance\"),\n",
    "        \"clarity\": _get(\"clarity\"),\n",
    "    }\n",
    "\n",
    "# Load the dataset (top-level list of records)\n",
    "with JSON_PATH.open(\"r\", encoding=\"utf-8\") as f:\n",
    "    records = json.load(f)\n",
    "if not isinstance(records, list):\n",
    "    raise ValueError(\"Expected a list at the top level of the JSON file.\")\n",
    "\n",
    "results = []  # collect a minimal summary if you want to use it later\n",
    "\n",
    "for idx, rec in enumerate(records, 1):\n",
    "    # Build the per-example payload expected by your teacher functions\n",
    "    example_local = {\n",
    "        \"id\": rec.get(\"id\"),\n",
    "        \"code\": rec.get(\"code\"),\n",
    "        \"question\": rec.get(\"question\"),\n",
    "        \"reference\": rec.get(\"reference\"),\n",
    "        \"prediction\": rec.get(\"prediction\"),\n",
    "    }\n",
    "    studentLLM_score_local = _extract_scores(rec)\n",
    "\n",
    "    # --- Your existing teacher logic ---\n",
    "    do_intervene = should_teacher_intervene(studentLLM_score_local, threshold=0.8)\n",
    "\n",
    "    teacher_feedbacks = {}\n",
    "    if do_intervene:\n",
    "        teacher_feedbacks = call_teacher_single_turn(\n",
    "            example_local[\"code\"],\n",
    "            example_local[\"question\"],\n",
    "            example_local[\"reference\"],\n",
    "            example_local[\"prediction\"],\n",
    "            studentLLM_score_local\n",
    "        )\n",
    "\n",
    "    # --- Compact printout (no “example” spam) ---\n",
    "    ex_id = example_local.get(\"id\", f\"#{idx}\")\n",
    "    if teacher_feedbacks:\n",
    "        print(f\"\\n[{ex_id}] feedback:\")\n",
    "        for dim, fb in teacher_feedbacks.items():\n",
    "            print(f\" - {dim}: {fb}\")\n",
    "    else:\n",
    "        print(f\"\\n[{ex_id}] no intervention.\" if not do_intervene else f\"\\n[{ex_id}] intervention, but no feedback returned.\")\n",
    "\n",
    "    # Save a tiny summary (optional)\n",
    "    results.append({\n",
    "        \"id\": ex_id,\n",
    "        \"intervene\": bool(do_intervene),\n",
    "        \"num_feedback_dims\": len(teacher_feedbacks),\n",
    "    })\n",
    "\n",
    "print(f\"\\nProcessed {len(records)} examples.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f46b8e60-86ef-469c-8719-20138bbe5e2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Student reflection for example q1 ===\n",
      "\n",
      " Student reflecting on teacher feedback...\n",
      "\n",
      " Revised Scores:\n",
      " {'accuracy': {'score': 2}, 'completeness': {'score': 2}, 'relevance': {'score': 3}, 'clarity': {'score': 2}}\n",
      "Final revised scores (Single Turn): {'accuracy': {'score': 2}, 'completeness': {'score': 2}, 'relevance': {'score': 3}, 'clarity': {'score': 2}}\n",
      "------------------------------------------------------------\n",
      "\n",
      "=== Student reflection for example q2 ===\n",
      "\n",
      " Student reflecting on teacher feedback...\n",
      "\n",
      " Revised Scores:\n",
      " {'accuracy': {'score': 2}, 'completeness': {'score': 2}, 'relevance': {'score': 3}, 'clarity': {'score': 3}}\n",
      "Final revised scores (Single Turn): {'accuracy': {'score': 2}, 'completeness': {'score': 2}, 'relevance': {'score': 3}, 'clarity': {'score': 3}}\n",
      "------------------------------------------------------------\n",
      "\n",
      "=== Student reflection for example q3 ===\n",
      "\n",
      " Student reflecting on teacher feedback...\n",
      "\n",
      " Revised Scores:\n",
      " {'accuracy': {'score': 2}, 'completeness': {'score': 2}, 'relevance': {'score': 3}, 'clarity': {'score': 3}}\n",
      "Final revised scores (Single Turn): {'accuracy': {'score': 2}, 'completeness': {'score': 2}, 'relevance': {'score': 3}, 'clarity': {'score': 3}}\n",
      "------------------------------------------------------------\n",
      "\n",
      "=== Student reflection for example q4 ===\n",
      "\n",
      " Student reflecting on teacher feedback...\n",
      "\n",
      " Revised Scores:\n",
      " {'accuracy': {'score': 2}, 'completeness': {'score': 2}, 'relevance': {'score': 3}, 'clarity': {'score': 2}}\n",
      "Final revised scores (Single Turn): {'accuracy': {'score': 2}, 'completeness': {'score': 2}, 'relevance': {'score': 3}, 'clarity': {'score': 2}}\n",
      "------------------------------------------------------------\n",
      "\n",
      "=== Student reflection for example q5 ===\n",
      "\n",
      " Student reflecting on teacher feedback...\n",
      "\n",
      " Revised Scores:\n",
      " {'accuracy': {'score': 2}, 'completeness': {'score': 1}, 'relevance': {'score': 3}, 'clarity': {'score': 2}}\n",
      "Final revised scores (Single Turn): {'accuracy': {'score': 2}, 'completeness': {'score': 1}, 'relevance': {'score': 3}, 'clarity': {'score': 2}}\n",
      "------------------------------------------------------------\n",
      "\n",
      "=== Student reflection for example q6 ===\n",
      "\n",
      " Student reflecting on teacher feedback...\n",
      "\n",
      " Revised Scores:\n",
      " {'accuracy': {'score': 2}, 'completeness': {'score': 2}, 'relevance': {'score': 3}, 'clarity': {'score': 3}}\n",
      "Final revised scores (Single Turn): {'accuracy': {'score': 2}, 'completeness': {'score': 2}, 'relevance': {'score': 3}, 'clarity': {'score': 3}}\n",
      "------------------------------------------------------------\n",
      "\n",
      "=== Student reflection for example q7 ===\n",
      "\n",
      " Student reflecting on teacher feedback...\n",
      "\n",
      " Revised Scores:\n",
      " {'accuracy': {'score': 2}, 'completeness': {'score': 1}, 'relevance': {'score': 3}, 'clarity': {'score': 3}}\n",
      "Final revised scores (Single Turn): {'accuracy': {'score': 2}, 'completeness': {'score': 1}, 'relevance': {'score': 3}, 'clarity': {'score': 3}}\n",
      "------------------------------------------------------------\n",
      "\n",
      "=== Student reflection for example q8 ===\n",
      "\n",
      " Student reflecting on teacher feedback...\n",
      "\n",
      " Revised Scores:\n",
      " {'accuracy': {'score': 2}, 'completeness': {'score': 2}, 'relevance': {'score': 3}, 'clarity': {'score': 3}}\n",
      "Final revised scores (Single Turn): {'accuracy': {'score': 2}, 'completeness': {'score': 2}, 'relevance': {'score': 3}, 'clarity': {'score': 3}}\n",
      "------------------------------------------------------------\n",
      "\n",
      "=== Student reflection for example q9 ===\n",
      "\n",
      " Student reflecting on teacher feedback...\n",
      "\n",
      " Revised Scores:\n",
      " {'accuracy': {'score': 2}, 'completeness': {'score': 2}, 'relevance': {'score': 3}, 'clarity': {'score': 3}}\n",
      "Final revised scores (Single Turn): {'accuracy': {'score': 2}, 'completeness': {'score': 2}, 'relevance': {'score': 3}, 'clarity': {'score': 3}}\n",
      "------------------------------------------------------------\n",
      "\n",
      "=== Student reflection for example q10 ===\n",
      "\n",
      " Student reflecting on teacher feedback...\n",
      "\n",
      " Revised Scores:\n",
      " {'accuracy': {'score': 2}, 'completeness': {'score': 2}, 'relevance': {'score': 3}, 'clarity': {'score': 2}}\n",
      "Final revised scores (Single Turn): {'accuracy': {'score': 2}, 'completeness': {'score': 2}, 'relevance': {'score': 3}, 'clarity': {'score': 2}}\n",
      "------------------------------------------------------------\n",
      "\n",
      "=== Student reflection for example q11 ===\n",
      "\n",
      " Student reflecting on teacher feedback...\n",
      "\n",
      " Revised Scores:\n",
      " {'accuracy': {'score': 2}, 'completeness': {'score': 2}, 'relevance': {'score': 3}, 'clarity': {'score': 3}}\n",
      "Final revised scores (Single Turn): {'accuracy': {'score': 2}, 'completeness': {'score': 2}, 'relevance': {'score': 3}, 'clarity': {'score': 3}}\n",
      "------------------------------------------------------------\n",
      "\n",
      "=== Student reflection for example q12 ===\n",
      "\n",
      " Student reflecting on teacher feedback...\n",
      "\n",
      " Revised Scores:\n",
      " {'accuracy': {'score': 2}, 'completeness': {'score': 1}, 'relevance': {'score': 3}, 'clarity': {'score': 2}}\n",
      "Final revised scores (Single Turn): {'accuracy': {'score': 2}, 'completeness': {'score': 1}, 'relevance': {'score': 3}, 'clarity': {'score': 2}}\n",
      "------------------------------------------------------------\n",
      "\n",
      "=== Student reflection for example q13 ===\n",
      "\n",
      " Student reflecting on teacher feedback...\n",
      "\n",
      " Revised Scores:\n",
      " {'accuracy': {'score': 2}, 'completeness': {'score': 2}, 'relevance': {'score': 3}, 'clarity': {'score': 3}}\n",
      "Final revised scores (Single Turn): {'accuracy': {'score': 2}, 'completeness': {'score': 2}, 'relevance': {'score': 3}, 'clarity': {'score': 3}}\n",
      "------------------------------------------------------------\n",
      "\n",
      "=== Student reflection for example q14 ===\n",
      "\n",
      " Student reflecting on teacher feedback...\n",
      "\n",
      " Revised Scores:\n",
      " {'accuracy': {'score': 2}, 'completeness': {'score': 2}, 'relevance': {'score': 3}, 'clarity': {'score': 3}}\n",
      "Final revised scores (Single Turn): {'accuracy': {'score': 2}, 'completeness': {'score': 2}, 'relevance': {'score': 3}, 'clarity': {'score': 3}}\n",
      "------------------------------------------------------------\n",
      "\n",
      "=== Student reflection for example q15 ===\n",
      "\n",
      " Student reflecting on teacher feedback...\n",
      "\n",
      " Revised Scores:\n",
      " {'accuracy': {'score': 2}, 'completeness': {'score': 2}, 'relevance': {'score': 3}, 'clarity': {'score': 3}}\n",
      "Final revised scores (Single Turn): {'accuracy': {'score': 2}, 'completeness': {'score': 2}, 'relevance': {'score': 3}, 'clarity': {'score': 3}}\n",
      "------------------------------------------------------------\n",
      "\n",
      "=== Student reflection for example q16 ===\n",
      "\n",
      " Student reflecting on teacher feedback...\n",
      "\n",
      " Revised Scores:\n",
      " {'accuracy': {'score': 2}, 'completeness': {'score': 2}, 'relevance': {'score': 3}, 'clarity': {'score': 3}}\n",
      "Final revised scores (Single Turn): {'accuracy': {'score': 2}, 'completeness': {'score': 2}, 'relevance': {'score': 3}, 'clarity': {'score': 3}}\n",
      "------------------------------------------------------------\n",
      "\n",
      "=== Student reflection for example q17 ===\n",
      "\n",
      " Student reflecting on teacher feedback...\n",
      "\n",
      " Revised Scores:\n",
      " {'accuracy': {'score': 2}, 'completeness': {'score': 2}, 'relevance': {'score': 3}, 'clarity': {'score': 3}}\n",
      "Final revised scores (Single Turn): {'accuracy': {'score': 2}, 'completeness': {'score': 2}, 'relevance': {'score': 3}, 'clarity': {'score': 3}}\n",
      "------------------------------------------------------------\n",
      "\n",
      "=== Student reflection for example q18 ===\n",
      "\n",
      " Student reflecting on teacher feedback...\n",
      "\n",
      " Revised Scores:\n",
      " {'accuracy': {'score': 2}, 'completeness': {'score': 2}, 'relevance': {'score': 3}, 'clarity': {'score': 2}}\n",
      "Final revised scores (Single Turn): {'accuracy': {'score': 2}, 'completeness': {'score': 2}, 'relevance': {'score': 3}, 'clarity': {'score': 2}}\n",
      "------------------------------------------------------------\n",
      "\n",
      "=== Student reflection for example q19 ===\n",
      "\n",
      " Student reflecting on teacher feedback...\n",
      "\n",
      " Revised Scores:\n",
      " {'accuracy': {'score': 2}, 'completeness': {'score': 2}, 'relevance': {'score': 3}, 'clarity': {'score': 3}}\n",
      "Final revised scores (Single Turn): {'accuracy': {'score': 2}, 'completeness': {'score': 2}, 'relevance': {'score': 3}, 'clarity': {'score': 3}}\n",
      "------------------------------------------------------------\n",
      "\n",
      "=== Student reflection for example q20 ===\n",
      "\n",
      " Student reflecting on teacher feedback...\n",
      "\n",
      " Revised Scores:\n",
      " {'accuracy': {'score': 2}, 'completeness': {'score': 2}, 'relevance': {'score': 3}, 'clarity': {'score': 2}}\n",
      "Final revised scores (Single Turn): {'accuracy': {'score': 2}, 'completeness': {'score': 2}, 'relevance': {'score': 3}, 'clarity': {'score': 2}}\n",
      "------------------------------------------------------------\n",
      "\n",
      "=== Student reflection for example q21 ===\n",
      "\n",
      " Student reflecting on teacher feedback...\n",
      "\n",
      " Revised Scores:\n",
      " {'accuracy': {'score': 2}, 'completeness': {'score': 2}, 'relevance': {'score': 3}, 'clarity': {'score': 3}}\n",
      "Final revised scores (Single Turn): {'accuracy': {'score': 2}, 'completeness': {'score': 2}, 'relevance': {'score': 3}, 'clarity': {'score': 3}}\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Student reflects and revises scores for each example\n",
    "\n",
    "for example_local, studentLLM_score_local in DATASET:\n",
    "    ex_id = example_local.get(\"id\")\n",
    "    print(f\"\\n=== Student reflection for example {ex_id} ===\")\n",
    "\n",
    "    # Make sure teacher_feedbacks were computed in Step 2 (or set default)\n",
    "    teacher_feedbacks_local = globals().get(\"teacher_feedbacks\", {}) or {}\n",
    "\n",
    "    revised_scores = student_reflect_and_revise(\n",
    "        example_local[\"code\"],\n",
    "        example_local[\"question\"],\n",
    "        example_local[\"reference\"],\n",
    "        example_local[\"prediction\"],\n",
    "        studentLLM_score_local,\n",
    "        teacher_feedbacks_local\n",
    "    )\n",
    "\n",
    "    print(\"Final revised scores (Single Turn):\", revised_scores)\n",
    "    print(\"-\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f364d17-6b91-4597-bd52-cde0b19d5542",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[q1] Q: What does the code make ?\n",
      "   prediction: Test Suite\n",
      "   reference : a suite\n",
      "   scores    : {'accuracy': 2, 'completeness': 1, 'relevance': 3, 'clarity': 2}\n",
      "------------------------------------------------------------\n",
      "[q2] Q: Does the code make a suite ?\n",
      "   prediction: Yes\n",
      "   reference : Yes\n",
      "   scores    : {'accuracy': 3, 'completeness': 3, 'relevance': 3, 'clarity': 3}\n",
      "------------------------------------------------------------\n",
      "[q3] Q: Does the code receive a message from a pull subscription ?\n",
      "   prediction: Yes, it receives messages from a pull subscription.\n",
      "   reference : Yes\n",
      "   scores    : {'accuracy': 3, 'completeness': 2, 'relevance': 3, 'clarity': 3}\n",
      "------------------------------------------------------------\n",
      "[q4] Q: What does the code receive from a pull subscription ?\n",
      "   prediction: Yes, messages\n",
      "   reference : a message\n",
      "   scores    : {'accuracy': 2, 'completeness': 1, 'relevance': 3, 'clarity': 2}\n",
      "------------------------------------------------------------\n",
      "[q5] Q: What be an explicit budget used only ?\n",
      "   prediction: Explicit Budget\n",
      "   reference : to create the campaign\n",
      "   scores    : {'accuracy': 2, 'completeness': 1, 'relevance': 3, 'clarity': 2}\n",
      "------------------------------------------------------------\n",
      "[q6] Q: Be an explicit budget used to create the campaign only ?\n",
      "   prediction: Yes\n",
      "   reference : Yes\n",
      "   scores    : {'accuracy': 3, 'completeness': 3, 'relevance': 3, 'clarity': 3}\n",
      "------------------------------------------------------------\n",
      "[q7] Q: What be used to create the campaign only ?\n",
      "   prediction: Explicit Budget\n",
      "   reference : an explicit budget\n",
      "   scores    : {'accuracy': 3, 'completeness': 2, 'relevance': 3, 'clarity': 3}\n",
      "------------------------------------------------------------\n",
      "[q8] Q: What does the code get ?\n",
      "   prediction: \n",
      "Plugins Directory Path\n",
      "   reference : the plugins directory path\n",
      "   scores    : {'accuracy': 2, 'completeness': 3, 'relevance': 3, 'clarity': 3}\n",
      "------------------------------------------------------------\n",
      "[q9] Q: Does the code get the plugins directory path ?\n",
      "   prediction: Yes, it does.\n",
      "   reference : Yes\n",
      "   scores    : {'accuracy': 3, 'completeness': 2, 'relevance': 3, 'clarity': 3}\n",
      "------------------------------------------------------------\n",
      "[q10] Q: What does the code write to a file ?\n",
      "   prediction: Yes, it writes an image file.\n",
      "   reference : an image\n",
      "   scores    : {'accuracy': 2, 'completeness': 1, 'relevance': 3, 'clarity': 2}\n",
      "------------------------------------------------------------\n",
      "[q11] Q: Does the code write an image to a file ?\n",
      "   prediction: Yes, it writes an image file\n",
      "   reference : Yes\n",
      "   scores    : {'accuracy': 3, 'completeness': 2, 'relevance': 3, 'clarity': 3}\n",
      "------------------------------------------------------------\n",
      "[q12] Q: What does the code add ?\n",
      "   prediction: Parameters\n",
      "   reference : parameters to url\n",
      "   scores    : {'accuracy': 2, 'completeness': 1, 'relevance': 3, 'clarity': 2}\n",
      "------------------------------------------------------------\n",
      "[q13] Q: Does the code add parameters to url ?\n",
      "   prediction: Yes, it adds parameters\n",
      "   reference : Yes\n",
      "   scores    : {'accuracy': 3, 'completeness': 2, 'relevance': 3, 'clarity': 3}\n",
      "------------------------------------------------------------\n",
      "[q14] Q: For what purpose do worksheet titles and ids read ?\n",
      "   prediction: Reads worksheet titles and ids for a workbook.\n",
      "   reference : for a workbook\n",
      "   scores    : {'accuracy': 3, 'completeness': 3, 'relevance': 3, 'clarity': 3}\n",
      "------------------------------------------------------------\n",
      "[q15] Q: Does the code raise an error if it ca nt be found ?\n",
      "   prediction: Yes\n",
      "   reference : Yes\n",
      "   scores    : {'accuracy': 3, 'completeness': 3, 'relevance': 3, 'clarity': 3}\n",
      "------------------------------------------------------------\n",
      "[q16] Q: What does the code raise if it ca nt be found ?\n",
      "   prediction: Command Execution Error\n",
      "   reference : an error\n",
      "   scores    : {'accuracy': 3, 'completeness': 2, 'relevance': 3, 'clarity': 3}\n",
      "------------------------------------------------------------\n",
      "[q17] Q: Does the code get a token if it ca nt be found ?\n",
      "   prediction: Yes, it can't find a token.\n",
      "   reference : Yes\n",
      "   scores    : {'accuracy': 3, 'completeness': 2, 'relevance': 3, 'clarity': 3}\n",
      "------------------------------------------------------------\n",
      "[q18] Q: What does the code get if it ca nt be found ?\n",
      "   prediction: \n",
      "Token was not configured\n",
      "   reference : a token\n",
      "   scores    : {'accuracy': 2, 'completeness': 3, 'relevance': 3, 'clarity': 1}\n",
      "------------------------------------------------------------\n",
      "[q19] Q: Are we using windows when ?\n",
      "   prediction:   Windows\n",
      "   reference : Yes\n",
      "   scores    : {'accuracy': 2, 'completeness': 1, 'relevance': 3, 'clarity': 2}\n",
      "------------------------------------------------------------\n",
      "[q20] Q: What are we using when ?\n",
      "   prediction: \n",
      "Yes (windows)\n",
      "   reference : windows\n",
      "   scores    : {'accuracy': 2, 'completeness': 1, 'relevance': 3, 'clarity': 2}\n",
      "------------------------------------------------------------\n",
      "[q21] Q: What does the code create ?\n",
      "   prediction: Empty URL conf 404 response\n",
      "   reference : an empty urlconf 404 error response\n",
      "   scores    : {'accuracy': 3, 'completeness': 2, 'relevance': 3, 'clarity': 3}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# === Added: Runner over DATASET ===\n",
    "def _default_run_example(_example, _scores):\n",
    "    # Fallback if your notebook doesn't define `run_example`\n",
    "    print(f\"[{_example.get('id')}] Q: {_example.get('question')}\")\n",
    "    print(f\"   prediction: {_example.get('prediction')}\")\n",
    "    print(f\"   reference : {_example.get('reference')}\")\n",
    "    print(f\"   scores    : {_scores}\")\n",
    "\n",
    "# Use notebook's `run_example` if it exists, otherwise fallback.\n",
    "_run_fn = globals().get(\"run_example\", _default_run_example)\n",
    "\n",
    "for _i, (_example, _scores) in enumerate(DATASET, 1):\n",
    "    _run_fn(_example, _scores)\n",
    "    if _i < len(DATASET):\n",
    "        print(\"-\" * 60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final runner (auto-sets `example` & `studentLLM_score` for each record)\n",
    "This cell loops over `DATASET`, sets `example` and `studentLLM_score` **as globals** (so legacy cells/functions work),\n",
    "and then calls your pipeline if available:\n",
    "\n",
    "- If `should_teacher_intervene` exists, it runs your teacher flow.\n",
    "- Else if `run_example` exists, it calls that.\n",
    "- Else, it prints a minimal fallback.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Running evaluation for example q1 (1/21) ===\n",
      "Intervention: True\n",
      "Feedbacks  : 0\n",
      "Scores     : {'accuracy': 2, 'completeness': 1, 'relevance': 3, 'clarity': 2}\n",
      "------------------------------------------------------------\n",
      "\n",
      "=== Running evaluation for example q2 (2/21) ===\n",
      "Intervention: False\n",
      "Feedbacks  : 0\n",
      "Scores     : {'accuracy': 3, 'completeness': 3, 'relevance': 3, 'clarity': 3}\n",
      "------------------------------------------------------------\n",
      "\n",
      "=== Running evaluation for example q3 (3/21) ===\n",
      "Intervention: False\n",
      "Feedbacks  : 0\n",
      "Scores     : {'accuracy': 3, 'completeness': 2, 'relevance': 3, 'clarity': 3}\n",
      "------------------------------------------------------------\n",
      "\n",
      "=== Running evaluation for example q4 (4/21) ===\n",
      "Intervention: True\n",
      "Feedbacks  : 0\n",
      "Scores     : {'accuracy': 2, 'completeness': 1, 'relevance': 3, 'clarity': 2}\n",
      "------------------------------------------------------------\n",
      "\n",
      "=== Running evaluation for example q5 (5/21) ===\n",
      "Intervention: True\n",
      "Feedbacks  : 0\n",
      "Scores     : {'accuracy': 2, 'completeness': 1, 'relevance': 3, 'clarity': 2}\n",
      "------------------------------------------------------------\n",
      "\n",
      "=== Running evaluation for example q6 (6/21) ===\n",
      "Intervention: False\n",
      "Feedbacks  : 0\n",
      "Scores     : {'accuracy': 3, 'completeness': 3, 'relevance': 3, 'clarity': 3}\n",
      "------------------------------------------------------------\n",
      "\n",
      "=== Running evaluation for example q7 (7/21) ===\n",
      "Intervention: False\n",
      "Feedbacks  : 0\n",
      "Scores     : {'accuracy': 3, 'completeness': 2, 'relevance': 3, 'clarity': 3}\n",
      "------------------------------------------------------------\n",
      "\n",
      "=== Running evaluation for example q8 (8/21) ===\n",
      "Intervention: False\n",
      "Feedbacks  : 0\n",
      "Scores     : {'accuracy': 2, 'completeness': 3, 'relevance': 3, 'clarity': 3}\n",
      "------------------------------------------------------------\n",
      "\n",
      "=== Running evaluation for example q9 (9/21) ===\n",
      "Intervention: False\n",
      "Feedbacks  : 0\n",
      "Scores     : {'accuracy': 3, 'completeness': 2, 'relevance': 3, 'clarity': 3}\n",
      "------------------------------------------------------------\n",
      "\n",
      "=== Running evaluation for example q10 (10/21) ===\n",
      "Intervention: True\n",
      "Feedbacks  : 0\n",
      "Scores     : {'accuracy': 2, 'completeness': 1, 'relevance': 3, 'clarity': 2}\n",
      "------------------------------------------------------------\n",
      "\n",
      "=== Running evaluation for example q11 (11/21) ===\n",
      "Intervention: False\n",
      "Feedbacks  : 0\n",
      "Scores     : {'accuracy': 3, 'completeness': 2, 'relevance': 3, 'clarity': 3}\n",
      "------------------------------------------------------------\n",
      "\n",
      "=== Running evaluation for example q12 (12/21) ===\n",
      "Intervention: True\n",
      "Feedbacks  : 0\n",
      "Scores     : {'accuracy': 2, 'completeness': 1, 'relevance': 3, 'clarity': 2}\n",
      "------------------------------------------------------------\n",
      "\n",
      "=== Running evaluation for example q13 (13/21) ===\n",
      "Intervention: False\n",
      "Feedbacks  : 0\n",
      "Scores     : {'accuracy': 3, 'completeness': 2, 'relevance': 3, 'clarity': 3}\n",
      "------------------------------------------------------------\n",
      "\n",
      "=== Running evaluation for example q14 (14/21) ===\n",
      "Intervention: False\n",
      "Feedbacks  : 0\n",
      "Scores     : {'accuracy': 3, 'completeness': 3, 'relevance': 3, 'clarity': 3}\n",
      "------------------------------------------------------------\n",
      "\n",
      "=== Running evaluation for example q15 (15/21) ===\n",
      "Intervention: False\n",
      "Feedbacks  : 0\n",
      "Scores     : {'accuracy': 3, 'completeness': 3, 'relevance': 3, 'clarity': 3}\n",
      "------------------------------------------------------------\n",
      "\n",
      "=== Running evaluation for example q16 (16/21) ===\n",
      "Intervention: False\n",
      "Feedbacks  : 0\n",
      "Scores     : {'accuracy': 3, 'completeness': 2, 'relevance': 3, 'clarity': 3}\n",
      "------------------------------------------------------------\n",
      "\n",
      "=== Running evaluation for example q17 (17/21) ===\n",
      "Intervention: False\n",
      "Feedbacks  : 0\n",
      "Scores     : {'accuracy': 3, 'completeness': 2, 'relevance': 3, 'clarity': 3}\n",
      "------------------------------------------------------------\n",
      "\n",
      "=== Running evaluation for example q18 (18/21) ===\n",
      "Intervention: False\n",
      "Feedbacks  : 0\n",
      "Scores     : {'accuracy': 2, 'completeness': 3, 'relevance': 3, 'clarity': 1}\n",
      "------------------------------------------------------------\n",
      "\n",
      "=== Running evaluation for example q19 (19/21) ===\n",
      "Intervention: True\n",
      "Feedbacks  : 0\n",
      "Scores     : {'accuracy': 2, 'completeness': 1, 'relevance': 3, 'clarity': 2}\n",
      "------------------------------------------------------------\n",
      "\n",
      "=== Running evaluation for example q20 (20/21) ===\n",
      "Intervention: True\n",
      "Feedbacks  : 0\n",
      "Scores     : {'accuracy': 2, 'completeness': 1, 'relevance': 3, 'clarity': 2}\n",
      "------------------------------------------------------------\n",
      "\n",
      "=== Running evaluation for example q21 (21/21) ===\n",
      "Intervention: False\n",
      "Feedbacks  : 0\n",
      "Scores     : {'accuracy': 3, 'completeness': 2, 'relevance': 3, 'clarity': 3}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# === FINAL DATASET RUNNER ===\n",
    "from pathlib import Path as _Path\n",
    "import json as _json\n",
    "\n",
    "def _extract_scores(_rec):\n",
    "    def _get_score(_k):\n",
    "        _v = _rec.get(_k, {})\n",
    "        if isinstance(_v, dict):\n",
    "            return int(_v.get(\"score\", 0))\n",
    "        if isinstance(_v, (int, float)):\n",
    "            return int(_v)\n",
    "        return 0\n",
    "    return {\n",
    "        \"accuracy\": _get_score(\"accuracy\"),\n",
    "        \"completeness\": _get_score(\"completeness\"),\n",
    "        \"relevance\": _get_score(\"relevance\"),\n",
    "        \"clarity\": _get_score(\"clarity\"),\n",
    "    }\n",
    "\n",
    "# Ensure DATASET exists; if not, build it from JSON\n",
    "try:\n",
    "    DATASET  # noqa: F821\n",
    "except NameError:\n",
    "    _JSON_PATH = _Path(\"/mnt/data/Llama3_CodeQA_llm_as_judge.json\")\n",
    "    with _JSON_PATH.open(\"r\", encoding=\"utf-8\") as _f:\n",
    "        _data = _json.load(_f)\n",
    "    if not isinstance(_data, list):\n",
    "        raise ValueError(\"Expected top-level list in Llama3_CodeQA_llm_as_judge.json\")\n",
    "    DATASET = []\n",
    "    for _rec in _data:\n",
    "        _example = {\n",
    "            \"id\": _rec.get(\"id\"),\n",
    "            \"code\": _rec.get(\"code\"),\n",
    "            \"question\": _rec.get(\"question\"),\n",
    "            \"reference\": _rec.get(\"reference\"),\n",
    "            \"prediction\": _rec.get(\"prediction\"),\n",
    "        }\n",
    "        _scores = _extract_scores(_rec)\n",
    "        DATASET.append((_example, _scores))\n",
    "    print(f\"[Final runner] Built DATASET with {len(DATASET)} examples.\")\n",
    "\n",
    "# Choose behavior based on what's defined in the notebook\n",
    "_has_teacher = \"should_teacher_intervene\" in globals()\n",
    "_has_feedback = \"provide_teacher_feedback\" in globals()\n",
    "_has_run_example = \"run_example\" in globals()\n",
    "\n",
    "for _idx, (_example, _scores) in enumerate(DATASET, 1):\n",
    "    # Set globals for legacy code expecting these names\n",
    "    globals()[\"example\"] = _example\n",
    "    globals()[\"studentLLM_score\"] = _scores\n",
    "\n",
    "    _ex_id = _example.get(\"id\", f\"#{_idx}\")\n",
    "    print(f\"\\n=== Running evaluation for example {_ex_id} ({_idx}/{len(DATASET)}) ===\")\n",
    "\n",
    "    try:\n",
    "        if _has_teacher:\n",
    "            # Execute your teacher pipeline\n",
    "            _do_intervene = should_teacher_intervene(_scores, threshold=0.8)\n",
    "            _teacher_feedbacks = 0\n",
    "            if _do_intervene and _has_feedback:\n",
    "                _teacher_feedbacks = provide_teacher_feedback(_example, _scores)\n",
    "\n",
    "            print(f\"Intervention: {_do_intervene}\")\n",
    "            print(f\"Feedbacks  : {_teacher_feedbacks}\")\n",
    "            print(f\"Scores     : {_scores}\")\n",
    "\n",
    "        elif _has_run_example:\n",
    "            # Fallback to your per-example function\n",
    "            run_example(_example, _scores)\n",
    "\n",
    "        else:\n",
    "            # Minimal fallback\n",
    "            print(f\"[{_ex_id}] Q: {_example.get('question')}\")\n",
    "            print(f\"   prediction: {_example.get('prediction')}\")\n",
    "            print(f\"   reference : {_example.get('reference')}\")\n",
    "            print(f\"   scores    : {_scores}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Example {_ex_id}: {e}\")\n",
    "\n",
    "    if _idx < len(DATASET):\n",
    "        print(\"-\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfbaa167",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
