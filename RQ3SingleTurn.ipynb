{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91fad3d2-06b2-4bfe-aadb-07c0cc48f11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import time\n",
    "from statistics import mean, stdev\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4134ca7c-0ac9-40ac-bd73-aaf78483ff35",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85da4fcb-5bff-4999-9710-6779a6e3d62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain.schema import SystemMessage, HumanMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3141636b-ca2d-4f2e-8a05-359cedbe6f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def should_teacher_intervene(scores, threshold=0.8):\n",
    "    \"\"\"\n",
    "    Determine if the teacher LLM should intervene based on scores.\n",
    "    \n",
    "    Args:\n",
    "        scores (dict): Dictionary with keys 'accuracy', 'completeness', 'relevance', 'clarity',\n",
    "                       each with integer score 1 to 3.\n",
    "        threshold (float): Penalty threshold for intervention.\n",
    "        \n",
    "    Returns:\n",
    "        bool: True if teacher should intervene, False otherwise.\n",
    "    \"\"\"\n",
    "    a = scores['accuracy']\n",
    "    c = scores['completeness']\n",
    "    r = scores['relevance']\n",
    "    l = scores['clarity']\n",
    "\n",
    "    # Compute weighted penalty\n",
    "    penalty = 0.5 * (3 - a) + 0.2 * (3 - c) + 0.15 * (3 - r) + 0.15 * (3 - l)\n",
    "\n",
    "    # Suspicious patterns\n",
    "    suspicious = (\n",
    "        (a == 1 and l == 3) or                          # Fluent but inaccurate\n",
    "        (a >= 2 and r == 1) or                          # Correct but off-topic\n",
    "        (a == 2 and r == 2 and l == 2) or               # Ambiguous scores\n",
    "        (a + r + l <= 5) or                             # Generally low scores\n",
    "        (abs(a - r) >= 2) or                            # Large inconsistency\n",
    "        (a == 3 and (r <= 1 or l <= 1))                 # Overconfident accuracy\n",
    "    )\n",
    "\n",
    "    # Decide intervention\n",
    "    return penalty > threshold or suspicious\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9a4a43-df27-4f3a-9d9d-c863e446991a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "\n",
    "STUDENT_MODEL = \"llama3.1:8b\"\n",
    "TEACHER_MODEL = \"llama3.1:70b\"\n",
    "SCORE_MAX = 3\n",
    "\n",
    "\n",
    "def call_llm_LangChain_correct_v2(prompt, temperature=0.0):\n",
    "    llm = ChatOllama(\n",
    "        model=\"llama3.1:8b\",\n",
    "        temperature=temperature\n",
    "    )\n",
    "\n",
    "    system_prompt = \"\"\"\n",
    "\n",
    "You are a large language model acting as a judge for assessing the performance of a Teaching Assistant (TA) in an introductory Python programming course.\n",
    "\n",
    "The TA is an LLM that answers student questions about Python code. Your job is to evaluate the quality of the TA's answer.\n",
    "\n",
    "You will receive:\n",
    "- A Python code snippet\n",
    "- A student question about that code\n",
    "- A reference (correct) answer\n",
    "- A TA LLM-generated answer (called the prediction)\n",
    "\n",
    "Your task is to evaluate how well the TA's prediction answers the student's question, using the following four dimensions. For each, provide:\n",
    "- An integer score from 1 to 3\n",
    "\n",
    "\n",
    "\n",
    "### Accuracy\n",
    "Compare the prediction with the reference to assess factual correctness and understanding of the code’s behavior and intent.\n",
    "You must judge whether the prediction reflects accurate behavior and matches core facts from the reference. \n",
    "You need to consider semantic meaning of code comprehension: understanding the structure, functionality, and intent behind the code.\\n\"\n",
    "\n",
    "Score meanings:\n",
    "- 1: Completely incorrect or irrelevant; does not address the reference answer.\n",
    "- 2: Partially correct; some key facts are accurate, but major details are wrong or missing.\n",
    "- 3: Fully correct; matches the reference answer in meaning and factual content.\n",
    "\n",
    "### Completeness\n",
    "Check if the prediction covers all important parts of the reference answer, including key concepts or conditions.\n",
    "\n",
    "Score meanings:\n",
    "- 1: Omits most key information or contains only a tiny fragment of relevant content.\n",
    "- 2: Covers some elements but misses important parts.\n",
    "- 3: Fully covers all essential information from the reference.\n",
    "\n",
    "### Relevance\n",
    "Assess whether the prediction directly addresses the question and stays on-topic.\n",
    "\n",
    "Score meanings:\n",
    "- 1: Completely irrelevant or mostly unrelated.\n",
    "- 2: Partially related but misses the main point.\n",
    "- 3: Fully focused and directly answers the question.\n",
    "\n",
    "### Clarity\n",
    "Evaluate how clearly and logically the prediction is expressed, ensuring it is easy to understand.\n",
    "\n",
    "Score meanings:\n",
    "- 1: Confusing, vague, or incoherent.\n",
    "- 2: Understandable but awkwardly phrased or slightly unclear.\n",
    "- 3: Clear, concise, and easy to follow.\n",
    "\n",
    "\n",
    "Example:\n",
    "\n",
    "Code:\n",
    "```python\n",
    "def count_even(nums):\n",
    "    total = 0\n",
    "    for x in nums:\n",
    "        if x % 2 == 0:\n",
    "            total += 1\n",
    "    return total\n",
    "Question: What does this function return when given a list of integers?\n",
    "Reference Answer: It returns the count of even numbers in the list.\n",
    "Prediction: It returns the count of odd numbers in the list.\n",
    "\n",
    "Evaluation Output:\n",
    "{\n",
    "\n",
    "\"accuracy\": { \"score\": 1 },\n",
    "\"completeness\": { \"score\": 1 },\n",
    "\"relevance\": { \"score\": 2 },\n",
    "\"clarity\": { \"score\": 3 }\n",
    "\n",
    "}\n",
    "\n",
    "Final Instructions:\n",
    "For the given input (code, question, reference answer, and prediction), evaluate the prediction on the four metrics defined above.\n",
    "Base your evaluation strictly on the content provided. Do not hallucinate missing information. Be consistent and objective.\n",
    "Do not include reasoning or explanations.\n",
    "\n",
    "Respond only with a JSON object in the exact format:\n",
    "{\n",
    "\"accuracy\": { \"score\": 1-3},\n",
    "\"completeness\": {\"score\": 1-3},\n",
    "\"relevance\": {\"score\": 1-3},\n",
    "\"clarity\": {\"score\": 1-3}\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "  \n",
    "\n",
    "    messages = [\n",
    "        SystemMessage(content=system_prompt),\n",
    "        HumanMessage(content=prompt)\n",
    "    ]\n",
    "\n",
    "    response = llm.invoke(messages)\n",
    "    return response.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d524b42-b629-4023-8d27-223ac20217c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_teacher_single_turn(code, question, reference, prediction, scores):\n",
    "    # Build teacher system prompt\n",
    "    system_prompt = \"\"\"\n",
    "You are a 70B teacher LLM model reviewing a student LLM-as-judge's evaluation of a Teaching Assistant's (TA) answer.\n",
    "\n",
    "You will receive:\n",
    "- Python code snippet\n",
    "- Student's question\n",
    "- Reference (correct) answer\n",
    "- TA's predicted answer\n",
    "- Scores assigned by the student LLM-as-judge\n",
    "\n",
    "Your task:\n",
    "- Examine the TA's predicted answer in context of the code, question, and reference.\n",
    "- For any dimension (Accuracy, Completeness, Relevance, Clarity) where the score is less than 3,\n",
    "  provide clear, concise feedback (2–4 sentences) explaining what could be improved.\n",
    "- If a dimension has no issues, do not include it in your response.\n",
    "\n",
    "Respond ONLY with a JSON object where keys are the dimension names (lowercase)\n",
    "and values are the feedback strings.\n",
    "\n",
    "Example output:\n",
    "{\n",
    "  \"accuracy\": \"The prediction misrepresents the function’s return value.\",\n",
    "  \"clarity\": \"The explanation lacks structure and is hard to follow.\"\n",
    "}\n",
    "\n",
    "Rubric:\n",
    "\n",
    "### Accuracy\n",
    "- 1: Completely incorrect or irrelevant.\n",
    "- 2: Partially correct but with major mistakes or omissions.\n",
    "- 3: Fully correct and matches the reference.\n",
    "\n",
    "### Completeness\n",
    "- 1: Omits most key information.\n",
    "- 2: Covers some but misses important parts.\n",
    "- 3: Fully covers all essential information.\n",
    "\n",
    "### Relevance\n",
    "- 1: Irrelevant or mostly unrelated.\n",
    "- 2: Partially related but misses main point.\n",
    "- 3: Fully focused and directly addresses the question.\n",
    "\n",
    "### Clarity\n",
    "- 1: Confusing, vague, or incoherent.\n",
    "- 2: Understandable but awkwardly phrased or unclear.\n",
    "- 3: Clear, concise, and easy to understand.\n",
    "\"\"\"\n",
    "\n",
    "    # Format input for the teacher\n",
    "    user_prompt = f\"\"\"\n",
    "```python\n",
    "{code}\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Reference Answer:\n",
    "{reference}\n",
    "\n",
    "TA's Predicted Answer:\n",
    "{prediction}\n",
    "\"\"\"\n",
    "\n",
    "    try:\n",
    "        llm = ChatOllama(model=TEACHER_MODEL, temperature=0.0)\n",
    "        messages = [\n",
    "            SystemMessage(content=system_prompt.strip()),\n",
    "            HumanMessage(content=user_prompt.strip())\n",
    "        ]\n",
    "        response = llm.invoke(messages)\n",
    "        critiques = json.loads(response.content.strip())\n",
    "        return critiques\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")\n",
    "        return {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5467df22-6626-40d0-a393-416770599575",
   "metadata": {},
   "outputs": [],
   "source": [
    "def student_reflect_and_revise(code, question, reference, prediction, old_score, critiques):\n",
    "    print(\"\\n Student reflecting on teacher feedback...\\n\")\n",
    "\n",
    "    critique_text = \"\\n\".join(\n",
    "        f\"{dim.upper()} Feedback: {critique.strip()}\" for dim, critique in critiques.items()\n",
    "    )\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "Code:\n",
    "```python\n",
    "{code}\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Reference Answer:\n",
    "{reference}\n",
    "\n",
    "TA's Predicted Answer:\n",
    "{prediction}\n",
    "\n",
    "Teacher's Feedback:\n",
    "{critique_text}\n",
    "\n",
    "Old prediction:\n",
    "{old_score}\n",
    "\n",
    "You have to update the old prediction by considering Teacher's Feedback. Please re-evaluate the TA's answer using:\n",
    "\n",
    "Accuracy\n",
    "\n",
    "Completeness\n",
    "\n",
    "Relevance\n",
    "\n",
    "Clarity\n",
    "\n",
    "Only return JSON:\n",
    "{{\n",
    "\"accuracy\": {{ \"score\": 1-3 }},\n",
    "\"completeness\": {{ \"score\": 1-3 }},\n",
    "\"relevance\": {{ \"score\": 1-3 }},\n",
    "\"clarity\": {{ \"score\": 1-3 }}\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "    try:\n",
    "        response = call_llm_LangChain_correct_v2(prompt, temperature=0.0)\n",
    "        revised_scores = json.loads(response)\n",
    "        print(\" Revised Scores:\\n\", revised_scores)\n",
    "    except Exception as e:\n",
    "        print(\" Failed to parse revised scores:\", str(e))\n",
    "        revised_scores = None\n",
    "\n",
    "    return revised_scores\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c9a95b-4a3c-44b6-97bf-529202b93784",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"### SINGLE-TURN PIPELINE ###\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86bdaef-c43a-4482-aa0e-c2de9f0cbbce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Student judge model outputs scores\n",
    "# The output is from CS1QA dataset on CodeLlama\n",
    "#)\n",
    "\n",
    "\n",
    "example = {\"id\": \"q132\",\n",
    "   \"code\": \"from cs1robots import* load_world(\\\"worlds/harvest1.wld\\\") #load world(\\\"worlds/harvest3.wld\\\") #load world(\\\"worlds/harvest4.wld\\\") hubo=Robot() hubo.set_trace('blue') def harvest(): if hubo.on_beeper(): hubo.pick_beeper() else: hubo.move() def right(): for i in range(3): hubo.turn_left() def right_up(): if hubo.on_beeper(): hubo.pick_beeper() hubo.turn_left() hubo.move() hubo.turn_left() def left_up(): right() hubo.move() right() def loop(): while hubo.front_is_clear(): harvest() right_up() while hubo.front_is_clear(): harvest() for i in range(2): loop() left_up() loop()\",\n",
    "    \"question\": \"In task4, do I have to pass only the beeper?\",\n",
    "    \"reference\": \"No, you don't need to, just pick up all the beepers!\",\n",
    "    \"prediction\": \"\\nHello! I understand that you are asking about a specific task related to programming in Python. However, your question is quite broad and doesn't provide enough context for me to fully understand what you need help with. Could you please provide more details or clarify your question so that I can better assist you?\",\n",
    "  }\n",
    "studentLLM_score = {\"accuracy\": 1, \"completeness\": 2, \"relevance\": 1, \"clarity\": 3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccef3ac5-24c5-4e55-a588-0851098ff0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Step 2: Teacher checks if intervention needed, then provides dimension-specific feedback\n",
    "do_intervene = should_teacher_intervene(studentLLM_score, threshold=0.8)\n",
    "\n",
    "teacher_feedbacks =0\n",
    "if (do_intervene):\n",
    "    teacher_feedbacks = call_teacher_single_turn(\n",
    "    example[\"code\"], example[\"question\"], example[\"reference\"], example[\"prediction\"], studentLLM_score\n",
    "    )\n",
    "\n",
    "\n",
    "# Print teacher feedbacks if any\n",
    "if teacher_feedbacks:\n",
    "    for dim, fb in teacher_feedbacks.items():\n",
    "        print(f\"\\n{dim}, feedback{fb}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46b8e60-86ef-469c-8719-20138bbe5e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#code, question, reference, prediction, critiques\n",
    "# Step 3: Student reflects and revises scores\n",
    "revised_scores = student_reflect_and_revise(example[\"code\"], example[\"question\"], example[\"reference\"], example[\"prediction\"], studentLLM_score, teacher_feedbacks)\n",
    "\n",
    "print(\"\\nFinal revised scores (Single Turn):\", revised_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f364d17-6b91-4597-bd52-cde0b19d5542",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
